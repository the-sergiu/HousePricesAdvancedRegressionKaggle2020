{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import Library \nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1- Exploratory data analysis**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read data from csv file\ndf_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of our training set: ',df_train.shape[0], 'houses', 'and', df_train.shape[1], 'features')\nprint('The shape of our testing set: ',df_test.shape[0], 'houses', 'and', df_test.shape[1], 'features')\nprint('The testing set has 1 feature less than the training set, which is SalePrice, the target to predict  ')","execution_count":4,"outputs":[{"output_type":"stream","text":"The shape of our training set:  1460 houses and 81 features\nThe shape of our testing set:  1459 houses and 80 features\nThe testing set has 1 feature less than the training set, which is SalePrice, the target to predict  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n\n  YrSold  SaleType  SaleCondition  SalePrice  \n0   2008        WD         Normal     208500  \n1   2007        WD         Normal     181500  \n2   2008        WD         Normal     223500  \n3   2006        WD        Abnorml     140000  \n4   2008        WD         Normal     250000  \n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 81 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n\n  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n\n  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n0       0      6    2010        WD         Normal  \n1   12500      6    2010        WD         Normal  \n2       0      3    2010        WD         Normal  \n3       0      6    2010        WD         Normal  \n4       0      1    2010        WD         Normal  \n\n[5 rows x 80 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1461</td>\n      <td>20</td>\n      <td>RH</td>\n      <td>80.0</td>\n      <td>11622</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>120</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1462</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>81.0</td>\n      <td>14267</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Gar2</td>\n      <td>12500</td>\n      <td>6</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1463</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>74.0</td>\n      <td>13830</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1464</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>78.0</td>\n      <td>9978</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1465</td>\n      <td>120</td>\n      <td>RL</td>\n      <td>43.0</td>\n      <td>5005</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>HLS</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>144</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 80 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" # Descriptive statistics "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\ncount  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \nmean    730.500000    56.897260    70.049958   10516.828082     6.099315   \nstd     421.610009    42.300571    24.284752    9981.264932     1.382997   \nmin       1.000000    20.000000    21.000000    1300.000000     1.000000   \n25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \nmax    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n\n       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\ncount  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \nmean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \nstd       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \nmin       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \nmax       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n\n        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\ncount  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \nmean     94.244521    46.660274      21.954110     3.409589    15.060959   \nstd     125.338794    66.256028      61.119149    29.317331    55.757415   \nmin       0.000000     0.000000       0.000000     0.000000     0.000000   \n25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n75%     168.000000    68.000000       0.000000     0.000000     0.000000   \nmax     857.000000   547.000000     552.000000   508.000000   480.000000   \n\n          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \ncount  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \nmean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \nstd      40.177307    496.123024     2.703626     1.328095   79442.502883  \nmin       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \nmax     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n\n[8 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1201.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1452.000000</td>\n      <td>1460.000000</td>\n      <td>...</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>730.500000</td>\n      <td>56.897260</td>\n      <td>70.049958</td>\n      <td>10516.828082</td>\n      <td>6.099315</td>\n      <td>5.575342</td>\n      <td>1971.267808</td>\n      <td>1984.865753</td>\n      <td>103.685262</td>\n      <td>443.639726</td>\n      <td>...</td>\n      <td>94.244521</td>\n      <td>46.660274</td>\n      <td>21.954110</td>\n      <td>3.409589</td>\n      <td>15.060959</td>\n      <td>2.758904</td>\n      <td>43.489041</td>\n      <td>6.321918</td>\n      <td>2007.815753</td>\n      <td>180921.195890</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>421.610009</td>\n      <td>42.300571</td>\n      <td>24.284752</td>\n      <td>9981.264932</td>\n      <td>1.382997</td>\n      <td>1.112799</td>\n      <td>30.202904</td>\n      <td>20.645407</td>\n      <td>181.066207</td>\n      <td>456.098091</td>\n      <td>...</td>\n      <td>125.338794</td>\n      <td>66.256028</td>\n      <td>61.119149</td>\n      <td>29.317331</td>\n      <td>55.757415</td>\n      <td>40.177307</td>\n      <td>496.123024</td>\n      <td>2.703626</td>\n      <td>1.328095</td>\n      <td>79442.502883</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>20.000000</td>\n      <td>21.000000</td>\n      <td>1300.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1872.000000</td>\n      <td>1950.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2006.000000</td>\n      <td>34900.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>365.750000</td>\n      <td>20.000000</td>\n      <td>59.000000</td>\n      <td>7553.500000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>1954.000000</td>\n      <td>1967.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>2007.000000</td>\n      <td>129975.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>730.500000</td>\n      <td>50.000000</td>\n      <td>69.000000</td>\n      <td>9478.500000</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>1973.000000</td>\n      <td>1994.000000</td>\n      <td>0.000000</td>\n      <td>383.500000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n      <td>2008.000000</td>\n      <td>163000.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1095.250000</td>\n      <td>70.000000</td>\n      <td>80.000000</td>\n      <td>11601.500000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n      <td>2000.000000</td>\n      <td>2004.000000</td>\n      <td>166.000000</td>\n      <td>712.250000</td>\n      <td>...</td>\n      <td>168.000000</td>\n      <td>68.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>2009.000000</td>\n      <td>214000.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1460.000000</td>\n      <td>190.000000</td>\n      <td>313.000000</td>\n      <td>215245.000000</td>\n      <td>10.000000</td>\n      <td>9.000000</td>\n      <td>2010.000000</td>\n      <td>2010.000000</td>\n      <td>1600.000000</td>\n      <td>5644.000000</td>\n      <td>...</td>\n      <td>857.000000</td>\n      <td>547.000000</td>\n      <td>552.000000</td>\n      <td>508.000000</td>\n      <td>480.000000</td>\n      <td>738.000000</td>\n      <td>15500.000000</td>\n      <td>12.000000</td>\n      <td>2010.000000</td>\n      <td>755000.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 38 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"                Id   MSSubClass  LotFrontage       LotArea  OverallQual  \\\ncount  1459.000000  1459.000000  1232.000000   1459.000000  1459.000000   \nmean   2190.000000    57.378341    68.580357   9819.161069     6.078821   \nstd     421.321334    42.746880    22.376841   4955.517327     1.436812   \nmin    1461.000000    20.000000    21.000000   1470.000000     1.000000   \n25%    1825.500000    20.000000    58.000000   7391.000000     5.000000   \n50%    2190.000000    50.000000    67.000000   9399.000000     6.000000   \n75%    2554.500000    70.000000    80.000000  11517.500000     7.000000   \nmax    2919.000000   190.000000   200.000000  56600.000000    10.000000   \n\n       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\ncount  1459.000000  1459.000000   1459.000000  1444.000000  1458.000000  ...   \nmean      5.553804  1971.357779   1983.662783   100.709141   439.203704  ...   \nstd       1.113740    30.390071     21.130467   177.625900   455.268042  ...   \nmin       1.000000  1879.000000   1950.000000     0.000000     0.000000  ...   \n25%       5.000000  1953.000000   1963.000000     0.000000     0.000000  ...   \n50%       5.000000  1973.000000   1992.000000     0.000000   350.500000  ...   \n75%       6.000000  2001.000000   2004.000000   164.000000   753.500000  ...   \nmax       9.000000  2010.000000   2010.000000  1290.000000  4010.000000  ...   \n\n        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\ncount  1458.000000  1459.000000  1459.000000    1459.000000  1459.000000   \nmean    472.768861    93.174777    48.313914      24.243317     1.794380   \nstd     217.048611   127.744882    68.883364      67.227765    20.207842   \nmin       0.000000     0.000000     0.000000       0.000000     0.000000   \n25%     318.000000     0.000000     0.000000       0.000000     0.000000   \n50%     480.000000     0.000000    28.000000       0.000000     0.000000   \n75%     576.000000   168.000000    72.000000       0.000000     0.000000   \nmax    1488.000000  1424.000000   742.000000    1012.000000   360.000000   \n\n       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \ncount  1459.000000  1459.000000   1459.000000  1459.000000  1459.000000  \nmean     17.064428     1.744345     58.167923     6.104181  2007.769705  \nstd      56.609763    30.491646    630.806978     2.722432     1.301740  \nmin       0.000000     0.000000      0.000000     1.000000  2006.000000  \n25%       0.000000     0.000000      0.000000     4.000000  2007.000000  \n50%       0.000000     0.000000      0.000000     6.000000  2008.000000  \n75%       0.000000     0.000000      0.000000     8.000000  2009.000000  \nmax     576.000000   800.000000  17000.000000    12.000000  2010.000000  \n\n[8 rows x 37 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1232.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1444.000000</td>\n      <td>1458.000000</td>\n      <td>...</td>\n      <td>1458.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n      <td>1459.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2190.000000</td>\n      <td>57.378341</td>\n      <td>68.580357</td>\n      <td>9819.161069</td>\n      <td>6.078821</td>\n      <td>5.553804</td>\n      <td>1971.357779</td>\n      <td>1983.662783</td>\n      <td>100.709141</td>\n      <td>439.203704</td>\n      <td>...</td>\n      <td>472.768861</td>\n      <td>93.174777</td>\n      <td>48.313914</td>\n      <td>24.243317</td>\n      <td>1.794380</td>\n      <td>17.064428</td>\n      <td>1.744345</td>\n      <td>58.167923</td>\n      <td>6.104181</td>\n      <td>2007.769705</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>421.321334</td>\n      <td>42.746880</td>\n      <td>22.376841</td>\n      <td>4955.517327</td>\n      <td>1.436812</td>\n      <td>1.113740</td>\n      <td>30.390071</td>\n      <td>21.130467</td>\n      <td>177.625900</td>\n      <td>455.268042</td>\n      <td>...</td>\n      <td>217.048611</td>\n      <td>127.744882</td>\n      <td>68.883364</td>\n      <td>67.227765</td>\n      <td>20.207842</td>\n      <td>56.609763</td>\n      <td>30.491646</td>\n      <td>630.806978</td>\n      <td>2.722432</td>\n      <td>1.301740</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1461.000000</td>\n      <td>20.000000</td>\n      <td>21.000000</td>\n      <td>1470.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1879.000000</td>\n      <td>1950.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2006.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1825.500000</td>\n      <td>20.000000</td>\n      <td>58.000000</td>\n      <td>7391.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>1953.000000</td>\n      <td>1963.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>318.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>2007.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2190.000000</td>\n      <td>50.000000</td>\n      <td>67.000000</td>\n      <td>9399.000000</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>1973.000000</td>\n      <td>1992.000000</td>\n      <td>0.000000</td>\n      <td>350.500000</td>\n      <td>...</td>\n      <td>480.000000</td>\n      <td>0.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n      <td>2008.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2554.500000</td>\n      <td>70.000000</td>\n      <td>80.000000</td>\n      <td>11517.500000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n      <td>2001.000000</td>\n      <td>2004.000000</td>\n      <td>164.000000</td>\n      <td>753.500000</td>\n      <td>...</td>\n      <td>576.000000</td>\n      <td>168.000000</td>\n      <td>72.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>2009.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2919.000000</td>\n      <td>190.000000</td>\n      <td>200.000000</td>\n      <td>56600.000000</td>\n      <td>10.000000</td>\n      <td>9.000000</td>\n      <td>2010.000000</td>\n      <td>2010.000000</td>\n      <td>1290.000000</td>\n      <td>4010.000000</td>\n      <td>...</td>\n      <td>1488.000000</td>\n      <td>1424.000000</td>\n      <td>742.000000</td>\n      <td>1012.000000</td>\n      <td>360.000000</td>\n      <td>576.000000</td>\n      <td>800.000000</td>\n      <td>17000.000000</td>\n      <td>12.000000</td>\n      <td>2010.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 37 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Header name Columns \ndf_train.columns","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition', 'SalePrice'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.columns","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# split training data into numeric and categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = df_train.select_dtypes(exclude='object')\ncategorical = df_train.select_dtypes(include='object')","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nNumber of numeric features : \",(len(numeric.axes[1])))\nprint(\"\\n\", numeric.axes[1])","execution_count":12,"outputs":[{"output_type":"stream","text":"\nNumber of numeric features :  38\n\n Index(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice'],\n      dtype='object')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nNumber of categorical features : \",(len(categorical.axes[1])))\nprint(\"\\n\", categorical.axes[1])","execution_count":13,"outputs":[{"output_type":"stream","text":"\nNumber of categorical features :  43\n\n Index(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition'],\n      dtype='object')\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## numerical features correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate the numeric features and check his relevance\n\nnum_corr = numeric.corr()\ntable = num_corr['SalePrice'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7f3286d714d0>","text/html":"<style  type=\"text/css\" >\n    #T_06283280_d869_11ea_bf80_0242ac130202row0_col0 {\n            background-color:  #008000;\n            color:  #f1f1f1;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row1_col0 {\n            background-color:  #2a972a;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row2_col0 {\n            background-color:  #3aa03a;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row3_col0 {\n            background-color:  #49a849;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row4_col0 {\n            background-color:  #4caa4c;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row5_col0 {\n            background-color:  #4eab4e;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row6_col0 {\n            background-color:  #4fac4f;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row7_col0 {\n            background-color:  #59b159;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row8_col0 {\n            background-color:  #5eb45e;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row9_col0 {\n            background-color:  #60b560;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row10_col0 {\n            background-color:  #64b764;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row11_col0 {\n            background-color:  #67b967;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row12_col0 {\n            background-color:  #69ba69;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row13_col0 {\n            background-color:  #6cbc6c;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row14_col0 {\n            background-color:  #7cc57c;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row15_col0 {\n            background-color:  #83c983;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row16_col0 {\n            background-color:  #89cc89;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row17_col0 {\n            background-color:  #8acc8a;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row18_col0 {\n            background-color:  #8bcd8b;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row19_col0 {\n            background-color:  #91d091;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row20_col0 {\n            background-color:  #94d294;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row21_col0 {\n            background-color:  #9dd79d;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row22_col0 {\n            background-color:  #9fd89f;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row23_col0 {\n            background-color:  #a8dda8;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row24_col0 {\n            background-color:  #b4e4b4;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row25_col0 {\n            background-color:  #b8e6b8;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row26_col0 {\n            background-color:  #c1ebc1;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row27_col0 {\n            background-color:  #c2ebc2;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row28_col0 {\n            background-color:  #ccf1cc;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row29_col0 {\n            background-color:  #cef2ce;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row30_col0 {\n            background-color:  #cff3cf;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row31_col0 {\n            background-color:  #cff3cf;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row32_col0 {\n            background-color:  #d0f3d0;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row33_col0 {\n            background-color:  #d0f3d0;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row34_col0 {\n            background-color:  #daf9da;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row35_col0 {\n            background-color:  #dcfadc;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row36_col0 {\n            background-color:  #e5ffe5;\n            color:  #000000;\n        }    #T_06283280_d869_11ea_bf80_0242ac130202row37_col0 {\n            background-color:  #e5ffe5;\n            color:  #000000;\n        }</style><table id=\"T_06283280_d869_11ea_bf80_0242ac130202\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >SalePrice</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row0\" class=\"row_heading level0 row0\" >SalePrice</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row0_col0\" class=\"data row0 col0\" >1.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row1\" class=\"row_heading level0 row1\" >OverallQual</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row1_col0\" class=\"data row1 col0\" >0.790982</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row2\" class=\"row_heading level0 row2\" >GrLivArea</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row2_col0\" class=\"data row2 col0\" >0.708624</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row3\" class=\"row_heading level0 row3\" >GarageCars</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row3_col0\" class=\"data row3 col0\" >0.640409</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row4\" class=\"row_heading level0 row4\" >GarageArea</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row4_col0\" class=\"data row4 col0\" >0.623431</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row5\" class=\"row_heading level0 row5\" >TotalBsmtSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row5_col0\" class=\"data row5 col0\" >0.613581</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row6\" class=\"row_heading level0 row6\" >1stFlrSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row6_col0\" class=\"data row6 col0\" >0.605852</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row7\" class=\"row_heading level0 row7\" >FullBath</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row7_col0\" class=\"data row7 col0\" >0.560664</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row8\" class=\"row_heading level0 row8\" >TotRmsAbvGrd</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row8_col0\" class=\"data row8 col0\" >0.533723</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row9\" class=\"row_heading level0 row9\" >YearBuilt</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row9_col0\" class=\"data row9 col0\" >0.522897</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row10\" class=\"row_heading level0 row10\" >YearRemodAdd</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row10_col0\" class=\"data row10 col0\" >0.507101</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row11\" class=\"row_heading level0 row11\" >GarageYrBlt</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row11_col0\" class=\"data row11 col0\" >0.486362</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row12\" class=\"row_heading level0 row12\" >MasVnrArea</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row12_col0\" class=\"data row12 col0\" >0.477493</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row13\" class=\"row_heading level0 row13\" >Fireplaces</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row13_col0\" class=\"data row13 col0\" >0.466929</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row14\" class=\"row_heading level0 row14\" >BsmtFinSF1</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row14_col0\" class=\"data row14 col0\" >0.386420</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row15\" class=\"row_heading level0 row15\" >LotFrontage</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row15_col0\" class=\"data row15 col0\" >0.351799</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row16\" class=\"row_heading level0 row16\" >WoodDeckSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row16_col0\" class=\"data row16 col0\" >0.324413</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row17\" class=\"row_heading level0 row17\" >2ndFlrSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row17_col0\" class=\"data row17 col0\" >0.319334</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row18\" class=\"row_heading level0 row18\" >OpenPorchSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row18_col0\" class=\"data row18 col0\" >0.315856</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row19\" class=\"row_heading level0 row19\" >HalfBath</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row19_col0\" class=\"data row19 col0\" >0.284108</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row20\" class=\"row_heading level0 row20\" >LotArea</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row20_col0\" class=\"data row20 col0\" >0.263843</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row21\" class=\"row_heading level0 row21\" >BsmtFullBath</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row21_col0\" class=\"data row21 col0\" >0.227122</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row22\" class=\"row_heading level0 row22\" >BsmtUnfSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row22_col0\" class=\"data row22 col0\" >0.214479</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row23\" class=\"row_heading level0 row23\" >BedroomAbvGr</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row23_col0\" class=\"data row23 col0\" >0.168213</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row24\" class=\"row_heading level0 row24\" >ScreenPorch</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row24_col0\" class=\"data row24 col0\" >0.111447</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row25\" class=\"row_heading level0 row25\" >PoolArea</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row25_col0\" class=\"data row25 col0\" >0.092404</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row26\" class=\"row_heading level0 row26\" >MoSold</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row26_col0\" class=\"data row26 col0\" >0.046432</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row27\" class=\"row_heading level0 row27\" >3SsnPorch</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row27_col0\" class=\"data row27 col0\" >0.044584</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row28\" class=\"row_heading level0 row28\" >BsmtFinSF2</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row28_col0\" class=\"data row28 col0\" >-0.011378</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row29\" class=\"row_heading level0 row29\" >BsmtHalfBath</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row29_col0\" class=\"data row29 col0\" >-0.016844</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row30\" class=\"row_heading level0 row30\" >MiscVal</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row30_col0\" class=\"data row30 col0\" >-0.021190</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row31\" class=\"row_heading level0 row31\" >Id</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row31_col0\" class=\"data row31 col0\" >-0.021917</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row32\" class=\"row_heading level0 row32\" >LowQualFinSF</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row32_col0\" class=\"data row32 col0\" >-0.025606</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row33\" class=\"row_heading level0 row33\" >YrSold</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row33_col0\" class=\"data row33 col0\" >-0.028923</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row34\" class=\"row_heading level0 row34\" >OverallCond</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row34_col0\" class=\"data row34 col0\" >-0.077856</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row35\" class=\"row_heading level0 row35\" >MSSubClass</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row35_col0\" class=\"data row35 col0\" >-0.084284</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row36\" class=\"row_heading level0 row36\" >EnclosedPorch</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row36_col0\" class=\"data row36 col0\" >-0.128578</td>\n            </tr>\n            <tr>\n                        <th id=\"T_06283280_d869_11ea_bf80_0242ac130202level0_row37\" class=\"row_heading level0 row37\" >KitchenAbvGr</th>\n                        <td id=\"T_06283280_d869_11ea_bf80_0242ac130202row37_col0\" class=\"data row37 col0\" >-0.135907</td>\n            </tr>\n    </tbody></table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_train=train.corr()\nsb.set(font_scale=2)\nplt.figure(figsize = (50,35))\nax = sb.heatmap(correlation_train, annot=True,annot_kws={\"size\": 25},fmt='.1f',cmap='PiYG', linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nf, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(df_train.corr(),annot=True, linewidths=.1, fmt= '.1f',ax=ax, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = df_train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2- Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data in Traing examples\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### before the cleaning data we combine training and test data in order to remain keep the same structure"},{"metadata":{},"cell_type":"markdown","source":"Clean and Edit Dataframes\nWe must combine train and test datasets. Because This processes are must be carried out together"},{"metadata":{"trusted":true},"cell_type":"code","source":"na = df_train.shape[0] #na is the number of rows of the original training set\nnb = df_test.shape[0]  #nb is the number of rows of the original test set\ny_train = df_train['SalePrice'].to_frame()\n#Combine train and test sets\nc1 = pd.concat((df_train, df_test), sort=False).reset_index(drop=True)\n#Drop the target \"SalePrice\" and Id columns\nc1.drop(['SalePrice'], axis=1, inplace=True)\nc1.drop(['Id'], axis=1, inplace=True)\nprint(\"Total size for train and test sets is :\",c1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##msv1 method to visualize missing values per columns\ndef msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3): \n    \"\"\"\n    SOURCE: https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()/len(data))/1.7, thresh+12.5, 'Columns with more than %s%s missing values' %(thresh, '%'), fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()/len(data))/1.7, thresh - 5, 'Columns with less than %s%s missing values' %(thresh, '%'), fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msv1(c1, 20, color=('silver', 'gold', 'lightgreen', 'skyblue', 'lightpink'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But before going any further, we start by cleaning the data from missing values. I set the threshold to 80% (red line), all columns with more than 80% missing values will be dropped."},{"metadata":{},"cell_type":"markdown","source":"First thing to do is get rid of the features with more than 80% missing values (figure above). \nFor example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 80%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 80% missing values."},{"metadata":{},"cell_type":"markdown","source":" Features with >80% missing values , we will drop "},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns (features ) with > 80% missing vales\nc=c1.dropna(thresh=len(c1)*0.8, axis=1)\nprint('We dropped ',c1.shape[1]-c.shape[1], ' features in the combined set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of the combined dataset after dropping features with more than 80% M.V.', c.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Now what do we do in combine data that contains less than 80% missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"allna = (c.isnull().sum() / len(c))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\n\ndef msv2(data, width=12, height=8, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n    \"\"\"\n    SOURCE: https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(width, height))\n\n    allna = (data.isnull().sum() / len(data))*100\n    tightout= 0.008*max(allna)\n    allna = allna.drop(allna[allna == 0].index).sort_values().reset_index()\n    mn= ax.barh(allna.iloc[:,0], allna.iloc[:,1], color=color, edgecolor=edgecolor)\n    ax.set_title('Missing values percentage per column', fontsize=15, weight='bold' )\n    ax.set_xlabel('Percentage', weight='bold', size=15)\n    ax.set_ylabel('Features with missing values', weight='bold')\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    for i in ax.patches:\n        ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2))+'%',\n            fontsize=10, fontweight='bold', color='grey')\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing values percentage per column with less than 80 % "},{"metadata":{"trusted":true},"cell_type":"code","source":"msv2(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Before  compelete cleaning the data, we zoom at the features with missing values, those missing values won't be treated equally. Some features have barely 1 or 2 missing values, we will use the forward fill method to fill them."},{"metadata":{},"cell_type":"markdown","source":" We isolate the missing values from the rest of the dataset to have a good idea of how to treat them "},{"metadata":{"trusted":true},"cell_type":"code","source":"NA=c[allna.index.to_list()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split them to:\n\n* Categorical features\n* Numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint('We have :',NAcat.shape[1],'categorical features with missing values')\nprint('We have :',NAnum.shape[1],'numerical features with missing values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, 18 categorical features and 10 numerical features to clean.\n\nWe start with the numerical features, first thing to do is have a look at them to learn more about their distribution and decide how to clean them:\nMost of the features are going to be filled with 0s because we assume that they don't exist, for example GarageArea, GarageCars with missing values are simply because the house lacks a garage.\nGarageYrBlt: Year garage was built can't be filled with 0s, so we fill with the median (1980)."},{"metadata":{},"cell_type":"markdown","source":"# Numerical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAnum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NANUM= NAnum.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNANUM = NANUM.style.background_gradient(cmap=cm)\nNANUM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MasVnrArea: Masonry veneer area in square feet, the missing data means no veneer so we fill with 0\nc['MasVnrArea']=c.MasVnrArea.fillna(0)\n#LotFrontage has 16% missing values. We fill with the median\nc['LotFrontage']=c.LotFrontage.fillna(c.LotFrontage.median())\n#GarageYrBlt:  Year garage was built, we fill the gaps with the median: 1980\nc['GarageYrBlt']=c[\"GarageYrBlt\"].fillna(1980)\n#For the rest of the columns: Bathroom, half bathroom, basement related columns and garage related columns:\n#We will fill with 0s because they just mean that the hosue doesn't have a basement, bathrooms or a garage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bb=c[allna.index.to_list()]\nnan=bb.select_dtypes(exclude='object')\nN= nan.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nN= N.style.background_gradient(cmap=cm)\nN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Categorical features:"},{"metadata":{},"cell_type":"markdown","source":"And we have 18 Categorical features with missing values:\nSome features have just 1 or 2 missing values, so we will just use the forward fill method because they are obviously values that can't be filled with 'None's Features with many missing values are mostly basement and garage related (same as in numerical features) so as we did with numerical features (filling them with 0s), we will fill the categorical missing values with \"None\"s assuming that the houses lack basements and garages"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAcat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of missing values per column in  Categorical features after the drop missing values with > 80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNAcat1 = NAcat1.style.background_gradient(cmap=cm)\nNAcat1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table above helps us to locate the categorical features with few missing values.\n\nWe start our cleaning with the features having just few missing value (1 to 4): We fill the gap with forward fill method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_cols = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\n\nfor col in c[fill_cols]:\n    c[col] = c[col].fillna(method='ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd=c[allna.index.to_list()]\nw=dd.select_dtypes(include='object')\na= w.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\na= a.style.background_gradient(cmap=cm)\na","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dealt already with small missing values or values that can't be filled with \"0\" such as Garage year built.\nThe rest of the features are mostly basement and garage related with 100s of missing values, \nwe will just fill 0s in the numerical features and 'None' in categorical features, assuming that the houses don't have basements, full bathrooms or garage"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will just 'None' in categorical features\n#Categorical missing values\nNAcols=c.columns\nfor col in NAcols:\n    if c[col].dtype == \"object\":\n        c[col] = c[col].fillna(\"None\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will just fill 0s in the numerical features \n#Numerical missing values\nfor col in NAcols:\n    if c[col].dtype != \"object\":\n        c[col]= c[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nFillNA=c[allna.index.to_list()]\n\n\n\nFillNAcat=FillNA.select_dtypes(include='object')\n\nFC= FillNAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nFC= FC.style.background_gradient(cmap=cm)\nFC\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FillNAnum=FillNA.select_dtypes(exclude='object')\n\nFM= FillNAnum.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nFM= FM.style.background_gradient(cmap=cm)\nFM\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- Feature engineering:\n\nSince the area is a very important variable,\n we will create a new feature \"TotalArea\"\n that sums the area of all the floors and the basement\n\n * Bathrooms: All the bathroom in the ground floor\n * Year average: The average of the sum of the year the house was built and the year the house was remodeled"},{"metadata":{"trusted":true},"cell_type":"code","source":"c.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c['TotalArea'] = c['TotalBsmtSF'] + c['1stFlrSF'] + c['2ndFlrSF'] + c['GrLivArea'] +c['GarageArea']\n\nc['Bathrooms'] = c['FullBath'] + c['HalfBath']*0.5 \n\nc['Year average']= (c['YearRemodAdd']+c['YearBuilt'])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFeature engineering is very important to improve the model's performance, I will start in this kernel just with the TotalArea, Bathrooms and average year features and will keep updating the kernel by creating new features"},{"metadata":{},"cell_type":"markdown","source":"# 4- Encoding categorical features:"},{"metadata":{},"cell_type":"markdown","source":"\n\n### 4.1 Numerical features:\nWe start with numerical features that are actually categorical, for example \"Month sold\", the values are from 1 to 12, each number is assigned to a month November is number 11 while March is number 3. 11 is just the order of the months and not a given value, so we convert the \"Month Sold\" feature to categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"#c['MoSold'] = c['MoSold'].astype(str)\nc['MSSubClass'] = c['MSSubClass'].apply(str)\nc['YrSold'] = c['YrSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\nc['HasBsmt'] = pd.Series(len(c['TotalBsmtSF']), index=c.index)\nc['HasBsmt'] = 0 \nc.loc[c['TotalBsmtSF']>0,'HasBsmt'] = 1\n\n#transform data\nc.loc[c['HasBsmt']==1,'TotalBsmtSF'] = np.log(c['TotalBsmtSF'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 One hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"cb=pd.get_dummies(c)\nprint(\"the shape of the original dataset\",c.shape)\nprint(\"the shape of the encoded dataset\",cb.shape)\nprint(\"We have \",cb.shape[1]- c.shape[1], 'new encoded features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done with the cleaning and feature engineering. Now, we split the combined dataset to the original train and test sets\nAfter Cleaning and feature engineering to apply outliers on training set "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTrain = cb[:na]  #na is the number of rows of the original training set\n                 \nTest = cb[na:]  #testset  after clean missing values and feature engineering and encoder  we do NOT apply outliers on it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train.shape)\nprint(y_train.shape)\nprint(Test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5- Outliers detection:"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Outliers visualization:\nThis part of the kernel will be a little bit messy. I didn't want to deal with the outliers in the combined dataset to keep the shape of the original train and test datasets. Dropping them would shift the location of the rows.\n\n* If you know a better solution to this, I will be more than happy to read your recommandations.\n\n* OK. So we go back to our original train dataset to visualize the important features / Sale price scatter plot to find outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=df_train['TotalBsmtSF'], y=df_train['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=df_train['1stFlrSF'], y=df_train['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=df_train['MasVnrArea'], y=df_train['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=df_train['GarageArea'], y=df_train['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=df_train['TotRmsAbvGrd'], y=df_train['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The outliers are the points in the right that have a larger area or value but a very low sale price. We localize those points by sorting their respective columns\n * Interesting! The outlier in \"basement\" and \"first floor\" features is the same as the first outlier in ground living area: The outlier with index number 1298."},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Outliers localization:"},{"metadata":{},"cell_type":"markdown","source":"We sort the columns containing the outliers shown in the graph, we will use the function head() to show the outliers: head(number of outliers or dots shown in each plot)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['GrLivArea'].sort_values(ascending=False).head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TotalBsmtSF'].sort_values(ascending=False).head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['MasVnrArea'].sort_values(ascending=False).head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['1stFlrSF'].sort_values(ascending=False).head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['GarageArea'].sort_values(ascending=False).head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TotRmsAbvGrd'].sort_values(ascending=False).head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can safely remove those points."},{"metadata":{"trusted":true},"cell_type":"code","source":"train=Train[(Train['GrLivArea'] < 4600) & (Train['MasVnrArea'] < 1500)]\n\nprint('We removed ',Train.shape[0]- train.shape[0],'outliers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do the same thing with \"SalePrice\" Target values column, we localize those outliers and make sure they are the right outliers to remove.\n\nThey both have the same price range as the detected outliers. So, we can safely drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"target=df_train[['SalePrice']]\ntarget.loc[1298]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.loc[523]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We gather all the outliers index positions and drop them from the target dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pos = [1298,523, 297, 581, 1190, 1061, 635, 197,1328, 495, 583, 313, 335, 249, 706]\npos = [1298,523, 297]\ntarget.drop(target.index[pos], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nP.S. I didn't drop all the outliers because dropping all of them led to a worst RMSE score. More investigation is needed to filter those outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We make sure that both train and target sets have the same row number after removing the outliers:')\nprint( 'Train: ',train.shape[0], 'rows')\nprint('Target:', target.shape[0],'rows')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'], color=('orchid'), alpha=0.5)\nplt.title('Area-Price plot with outliers',weight='bold', fontsize=18)\nplt.axvline(x=4600, color='r', linestyle='-')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.scatter(x=train['GrLivArea'], y=target['SalePrice'], color='navy', alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Area-Price plot without outliers',weight='bold', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Log transform skewed numeric features:\nLog transform skewed numeric features:\nWe want our skewness value to be around 0 and kurtosis less than 3. For more information about skewness and kurtosis,I recommend reading this article.\n\nHere are two examples of skewed features: Ground living area and 1st floor SF. We will apply np.log1p to the skewed variables."},{"metadata":{},"cell_type":"markdown","source":"NOT Skewness basically gives the shape of normal distribution of values."},{"metadata":{},"cell_type":"markdown","source":"If skewness value lies above +1 or below -1, data is highly skewed. If it lies between +0.5 to -0.5, it is moderately skewed. If the value is 0, then the data is symmetric"},{"metadata":{},"cell_type":"markdown","source":"### Positively skewed data:\nIf tail is on the right as that of the second image in the figure, it is right skewed data. It is also called positive skewed data."},{"metadata":{},"cell_type":"markdown","source":"### Negatively skewed data:\nIf the tail is to the left of data, then it is called left skewed data. It is also called negatively skewed data."},{"metadata":{},"cell_type":"markdown","source":"[codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness before log transform: \", df_train['GrLivArea'].skew())\nprint(\"Kurtosis before log transform: \", df_train['GrLivArea'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew\n\n#numeric_feats = c.dtypes[c.dtypes != \"object\"].index\n\n#skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n#skewed_feats\n#skewed_feats = skewed_feats[skewed_feats > 0.75]\n#skewed_feats = skewed_feats.index\n\n#train[skewed_feats] = np.log1p(train[skewed_feats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness after log transform: \", train['GrLivArea'].skew())\nprint(\"Kurtosis after log transform: \", train['GrLivArea'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,10))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((2,2),(0,0))\nsns.distplot(df_train.GrLivArea, color='plum')\nplt.title('Before: Distribution of GrLivArea',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(0,1))\nsns.distplot(df_train['1stFlrSF'], color='tan')\nplt.title('Before: Distribution of 1stFlrSF',weight='bold', fontsize=18)\n\n\nax1 = plt.subplot2grid((2,2),(1,0))\nsns.distplot(train.GrLivArea, color='plum')\nplt.title('After: Distribution of GrLivArea',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(1,1))\nsns.distplot(train['1stFlrSF'], color='tan')\nplt.title('After: Distribution of 1stFlrSF',weight='bold', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last thing to do before Machine Learning is to log transform the target as well, as we did with the skewed features.\n\nP.S. Log transoform is only applied on the target in this version, not on the features. I will be applying the log transoform on the features in future versions of this kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"#histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness before log transform: \", target['SalePrice'].skew())\nprint(\"Kurtosis before log transform: \",target['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log transform the target:\ntarget[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transformed histogram and normal probability plot\nsns.distplot(target[\"SalePrice\"], fit=norm);\nfig = plt.figure()\nres = stats.probplot(target[\"SalePrice\"], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.hist(df_train.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.title('Sale price distribution before normalization',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.hist(target.SalePrice, bins=10, color='darkcyan',alpha=0.5)\nplt.title('Sale price distribution after normalization',weight='bold', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness after log transform: \", target['SalePrice'].skew())\nprint(\"Kurtosis after log transform: \",target['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The skewness and kurtosis values look fine after log transform. We can now move forward to Machine Learning.\n\nP.S.To get our original SalePrice values back, we will apply\n###  np.expm1\nat the end of the study to cancel the log1p transformation after training and testing the models."},{"metadata":{},"cell_type":"markdown","source":" Note!! the clean from missing values and apply  Feature Engineering  Required Combine between train and test sets\n and and clean outliers apply only training set"},{"metadata":{},"cell_type":"markdown","source":"# 6- Machine Learning:"},{"metadata":{},"cell_type":"markdown","source":"#### Note: We start machine learning by setting the features and target:\n\n* Features: x\n* Target: y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train\ny=np.array(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)    # features variable afte clean and feature engineering and clean outliers\nprint(y.shape)    # target varible afte clean and and scaling using log transform\nprint(Test.shape) # test set after cleaning and feature engineering but Do not apply clean outliers and Scaling on it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nimport math\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Then, we split them to train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y,test_size = .3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We use RobustScaler to scale our data because it's powerful against outliers\n#### we already detected some but there must be some other outliers out there"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler= RobustScaler()\n# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n\n#Transform the test set\nX_test= scaler.transform(Test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We first start by trying the very basic regression model: Linear regression.\n\n##### We use 5- Fold cross validation for a better error estimate:"},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlreg=LinearRegression()\nMSEs=ms.cross_val_score(lreg,X , y, scoring='neg_mean_squared_error', cv=5)\nmeanMSE=np.mean(MSEs)\nprint(meanMSE)\nprint('RMSE = '+str(math.sqrt(-meanMSE)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Our goal is to minimize the error, we use regularization methods: Ridge, Lasso and ElasticNet, in order to lower the squared error"},{"metadata":{},"cell_type":"markdown","source":"### 6.3 Regularization:"},{"metadata":{},"cell_type":"markdown","source":"#### Ridge regression: \n      * Minimize squared error + a term alpha that penalizes the error\n      * We need to find a value of alpha that minimizes the train and test error (avoid overfitting)\n      * it use L2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.model_selection as GridSearchCV # to get more great Hypert pramater value\nfrom sklearn.linear_model import Ridge\n\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nridge_reg.fit(x_train,y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)\nprint(\"The best score achieved with Alpha=11 is: \",math.sqrt(-ridge_reg.best_score_))\nridge_pred=math.sqrt(-ridge_reg.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_mod=Ridge(alpha=20)\nridge_mod.fit(x_train,y_train)\ny_pred_train=ridge_mod.predict(x_train)\ny_pred_test=ridge_mod.predict(x_test)\n\nprint('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test))))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lasso regression\n* Next we try Lasso regularization: Similar procedure as ridge regularization but Lasso tends to have a lot of 0 entries in it and just few nonzeros (easy selection). * In other words, lasso drops the uninformative features and keeps just the important ones.\n* As with Ridge regularization, we need to find the alpha parameter that penalizes the error\n* it use L1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nparameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\n\nlasso=Lasso()\nlasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nlasso_reg.fit(x_train,y_train)\n\nprint('The best value of Alpha is: ',lasso_reg.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_mod=Lasso(alpha=0.0009)\nlasso_mod.fit(x_train,y_train)\ny_lasso_train=lasso_mod.predict(x_train)\ny_lasso_test=lasso_mod.predict(x_test)\n\nprint('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_train, y_lasso_train))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_lasso_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We check next, the important features that our model used to make predictions\n* The number of uninformative features that were dropped. Lasso give a 0 coefficient to the useless features, we will use the coefficient given to the important feature to plot the graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = pd.Series(lasso_mod.coef_, index = X.columns)\n\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\", color='yellowgreen')\nplt.xlabel(\"Lasso coefficient\", weight='bold')\nplt.title(\"Feature importance in the Lasso Model\", weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! The most important feature is the new feature we created \"TotalArea\".\n\nOther features such as neighborhood or overall quality are among the main important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Lasso kept \",sum(coefs != 0), \"important features and dropped the other \", sum(coefs == 0),\" features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we try ElasticNet. A regressor that combines both ridge and Lasso. We use cross validation to find:\n\nAlpha\nRatio between Ridge and Lasso, for a better combination of both"},{"metadata":{},"cell_type":"markdown","source":"### ElasticNet:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\nalphas = [10,1,0.1,0.01,0.001,0.002,0.003,0.004,0.005,0.00054255]\nl1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n\nelastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\nelasticmod = elastic_cv.fit(x_train, y_train.ravel())\nela_pred=elasticmod.predict(x_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))\nprint(elastic_cv.alpha_)\nprint(elastic_cv.l1_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#REGULARIZATION RECAP","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## REGULARIZATION RECAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y.shape)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### In regularization we worked with 3 algorithms: Ridge (L2), Lasso (L1) and ElasticNet that is a combination of both L2 and L1 regressors. \n##### Before moving to the next section of this work, I would like to introduce a function that does all the work we did above in details just with one line of code. \n##### The function does all the regression pipeline:"},{"metadata":{},"cell_type":"markdown","source":"1-Split the data to train/test\n2-Scale the data\n3-Gridsearch for the best hyperparameters\n4-Predict the target\n5-Evaluate the prediction\n\n##### The function takes as input parameters:\n\n* x: the features\n* y: the target\n* modelo: Ridge(default), Lasso, ElasticNetCV\n* scaler: RobustScaler(default), MinMaxScaler, StandardScaler"},{"metadata":{},"cell_type":"markdown","source":"define some helper functions, which would be used repeatedly"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a function that would split data into training and testing\ndef split_data(features, target):\n    \n    from sklearn.model_selection import train_test_split\n    \n    x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=1)\n    \n    return x_train, x_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function to evaluate different scores\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\ndef get_score(y_test, y_pred):\n    r2 = r2_score(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    rmsle = mean_squared_log_error(y_test, y_pred) \n    return (r2, mse, rmse, rmsle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to Plot the Actual vs Predicted Sale Prices\ndef plot_data(y_test, y_pred):\n    plt.figure(figsize=(18,12))\n    plt.plot(y_test.values, label='Actual', c='r')\n    plt.plot(y_pred, label='Predicted', c='b')\n    plt.title('Actual vs Predicted Sale Price of the House')\n    plt.ylabel('Sale Price')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regularization(x,y,modelo=Ridge, scaler=RobustScaler):\n    \"\"\"\"\n    Function to automate regression with regularization techniques.\n    x expects the features\n    y expects the target\n    modelo: Ridge(default), Lasso, ElasticNetCV\n    scaler: RobustScaler(default), MinMaxSclaer, StandardScaler\n    SOURCE: https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n    Contact: amineyamlahi@gmail.com\n    \"\"\"\n    #Split the data to train/test\n    from sklearn.model_selection import train_test_split\n    x_train, x_test, y_train, y_test = train_test_split(x, y,test_size = .3, random_state=0)\n    \n    #Scale the data. RobustSclaer default\n    from sklearn.preprocessing import RobustScaler\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import StandardScaler\n    \n    scaler= scaler()\n    # transform \"x_train\"\n    x_train = scaler.fit_transform(x_train)\n    # transform \"x_test\"\n    x_test = scaler.transform(x_test)\n    #Transform the test set\n    X_test= scaler.transform(Test)\n    \n    if modelo != ElasticNetCV:\n        if modelo == Ridge:\n            parameters= {'alpha':[x for x in range(1,101)]}\n        elif modelo == Lasso:\n            parameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n            \n        model=modelo()\n            \n        model=ms.GridSearchCV(model, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\n        model.fit(x_train,y_train)\n        y_pred= model.predict(x_test)\n\n        #print(\"The best value of Alpha is: \",model.best_params_)\n        print(\"The best RMSE score achieved with %s is: %s \" %(model.best_params_,\n                  str(math.sqrt(sklm.mean_squared_error(y_test, y_pred)))))\n    elif modelo == ElasticNetCV:\n        alphas = [0.000542555]\n        l1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n\n        elastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\n        elasticmod = elastic_cv.fit(x_train, y_train.ravel())\n        ela_pred=elasticmod.predict(x_test)\n        print(\"The best RMSE score achieved with alpha %s and l1_ratio %s is: %s \"\n              %(elastic_cv.alpha_,elastic_cv.l1_ratio_,\n            str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred)))))\n        \n            \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.4 XGB and ExtraTrees regressors:"},{"metadata":{},"cell_type":"markdown","source":"We will try other kind of regressors, such as XGBRegressor and ExtraTreesRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBRegressor\n\n#xg_reg = XGBRegressor()\n#xgparam_grid= {'learning_rate' : [0.01],'n_estimators':[2000, 3460, 4000],\n#                                     'max_depth':[3], 'min_child_weight':[3,5],\n#                                     'colsample_bytree':[0.5,0.7],\n#                                     'reg_alpha':[0.0001,0.001,0.01,0.1,10,100],\n#                                    'reg_lambda':[1,0.01,0.8,0.001,0.0001]}\n\n#xg_grid=GridSearchCV(xg_reg, param_grid=xgparam_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n#xg_grid.fit(x_train,y_train)\n#print(xg_grid.best_estimator_)\n#print(xg_grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb= XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=3, min_child_weight=0, missing=None, n_estimators=4000,\n             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n             reg_alpha=0.0001, reg_lambda=0.01, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nxgmod=xgb.fit(x_train,y_train)\nxg_pred=xgmod.predict(x_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, xg_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.5 ENSEMBLE METHODS:\n   * VOTING REGRESSOR:\n   \n        ** A voting regressor is an ensemble meta-estimator that fits base regressors each on the whole dataset. It, then, averages the individual predictions to form                 a final prediction.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), \n                            ('XGBRegressor', xgb)])\nvote= vote_mod.fit(x_train, y_train.ravel())\nvote_pred=vote.predict(x_test)\n\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, vote_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.5 Gradient Boosted Regressor"},{"metadata":{},"cell_type":"markdown","source":"# baseline Gradient Boosted Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor()\n\nx_train, x_test, y_train, y_test = split_data(X, y)\n\ngbr_model.fit(x_train, y_train)"},{"metadata":{},"cell_type":"markdown","source":"y_pred = gbr_model.predict(x_test)\n\nr2, mse, rmse, rmsle = get_score(y_test, y_pred)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred))))\n\n"},{"metadata":{},"cell_type":"markdown","source":"Grid Search for hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# warm_start = store and reuse previous fit values\n# n_iter_no_change = used for early stopping, terminates the training process if validation score is not improving for this many iterations\ngrb = GradientBoostingRegressor(warm_start=True, n_iter_no_change=15)\nx_train, x_test, y_train, y_test = split_data(X, y)\n\nnum_estimators = [100, 250, 500]\nlearn_rates = [0.05, 0.1]\ndepths = [3, 5, None]\n\n# test different values of n_estimators to see which one would yield better results\nparams = {'n_estimators': num_estimators, 'max_depth': depths, 'learning_rate': learn_rates}\n\n# cv = 5 -> 5-fold Cross Validation\ngrid_search = GridSearchCV(estimator=grb, param_grid=params, cv=5, return_train_score=True)\ngrid_search.fit(x_train, y_train)"},{"metadata":{},"cell_type":"markdown","source":"# get the best parameters\ngrid_search.best_params_"},{"metadata":{},"cell_type":"markdown","source":"# let's view each model/parameters in detail\n# total_size = 3*2*3 = 18\nfor i in range(18):\n    print('Parameters:', grid_search.cv_results_['params'][i])\n    print('Mean Test Score:', grid_search.cv_results_['mean_test_score'][i])\n    print('Rank', grid_search.cv_results_['rank_test_score'][i])"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gradient Boosting Regressor with Hyperparameters as per Grid Search","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Regressor with Hyperparameters as per Grid Search\n\nhyper_param = grid_search.best_params_\n\ngradient_reg = GradientBoostingRegressor(n_estimators=hyper_param['n_estimators'], max_depth=hyper_param['max_depth'],\n                                         learning_rate=hyper_param['learning_rate'])\n\nx_train, x_test, y_train, y_test = split_data(X, y)\n\ngradient_reg.fit(x_train, y_train)"},{"metadata":{},"cell_type":"markdown","source":"y_pred = gradient_reg.predict(x_test)\n\nr2, mse, rmse, rmsle = get_score(y_test, y_pred)\n\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred))))\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STACKING REGRESSOR:"},{"metadata":{},"cell_type":"markdown","source":"We stack all the previous models, including the votingregressor with XGBoost as the meta regressor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.regressor import StackingRegressor\n\n\nstregr = StackingRegressor(regressors=[elastic_cv,ridge_mod, lasso_mod, vote_mod], \n                           meta_regressor=xgb, use_features_in_secondary=True\n                          )\n\nstack_mod=stregr.fit(x_train, y_train.ravel())\nstacking_pred=stack_mod.predict(x_test)\n\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, stacking_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last thing to do is average our regressors and fit them on the testing dataset"},{"metadata":{},"cell_type":"markdown","source":"#### Averaging Regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test=(0.3*vote_pred+0.5*stacking_pred+ 0.2*y_lasso_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, final_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Averaging the 3 best models: Stacking, Voting and Lasso gave the best results: The lowest RMSE\n\nThe coefficients assigned to the 3 models were tested manually, the models combination above gave the best RMSE score"},{"metadata":{},"cell_type":"markdown","source":"### 6.6 Fit the model on test data"},{"metadata":{},"cell_type":"markdown","source":"Now, we fit the models on the test data and then submit it to the competition\n\n  * We apply np.expm1 to cancel the np.logp1 (we did previously in data processing) and convert the numbers to their original form "},{"metadata":{"trusted":true},"cell_type":"code","source":"#VotingRegressor to predict the final Test\nvote_test = vote_mod.predict(X_test)\nfinal1=np.expm1(vote_test)\n\n#StackingRegressor to predict the final Test\nstack_test = stregr.predict(X_test)\nfinal2=np.expm1(stack_test)\n\n#LassoRegressor to predict the final Test\nlasso_test = lasso_mod.predict(X_test)\nfinal3=np.expm1(lasso_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submission of the results predicted by the average of Voting/Stacking/Lasso\nfinal=(0.2*final1+0.6*final2+0.2*final3)\n\nfinal_submission = pd.DataFrame({\n        \"Id\": df_test[\"Id\"],\n        \"SalePrice\": final\n    })\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}